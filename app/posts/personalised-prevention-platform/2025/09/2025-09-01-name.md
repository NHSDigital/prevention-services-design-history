---
title: "Presenting next steps"
description: "Presented with useful and relevant options, do people feel encouraged to take up activities?"
date: 2025-09-01
tags:
- prototyping
---


Hello! We’ve undergone a mild rebrand. We’re now known as the Weight Management team, still under Personalised Prevention Services.
{.nhsuk-body-l}

Prior to this we were the [Personalised Prevention Platform (PPP).](/personalised-prevention-platform/)

A lot of our underlying thinking remains the same:

1. we can reach the right people
2. we can encourage people to act to improve their health
3. we can help people to discover services that meet their needs
4. we can understand enough about the user to suggest what might be effective
5. we have permission for a life-long dialogue

Our thin steel thread is a prevention journey for weight management.

Our [previous post](/personalised-prevention-platform/2025/04/onboarding-users/) talked about how we've been exploring how we introduce our service to users, and find out a bit about them, through an “onboarding” process.

In this post let’s look at the next stage: presenting next steps.

## Why present next steps?

Everyone we have spoken to told us they believed they could be doing more to maintain their health.

> [!NOTE]
> However they remained largely unaware of the range of help **already** available to them.

We are confident that any given individual can be presented with a range of options that could be well suited to them. For example:

* solo self-directed apps
* in-person group programmes
* regular community events

We wanted to test how we might present next steps in a clear and actionable manner.

Our prototypes were rooted in four underlying themes:

### 1. What’s the value of presenting a range of opportunities?

Personalisation can only ever go so far &ndash; particularly in the case of [primary prevention.](http://localhost:8080/personalised-prevention/2025/07/definitions/) It is not realistic or desirable to assume that we can propose one perfect route for every individual &ndash; choice and agency must play a part.

Can we provide:

* enough choice for a user to find at least one interesting thing?
* a narrow enough selection to avoid overwhelm and choice paralysis?

### 2. What’s the minimum viable information about an activity we can use?

It is not news to state that directories of services represent “hard yards”. The underlying work of assembling and maintaining information about services (of all shapes and sizes) has been repeatedly “discovered”.

However it’s also not news to state that information such as this is required to underpin all kinds of transformational capabilities (not just a weight management journey).

In our case we need to work out how to establish a source of information about the options we might present to a user in a pilot area.

We’ve made some experimental inroads with some help from the AI Health Coach team (thank you!), asking can algos and agents:

* rapidly assemble a “starter for 10” of relevant local services based on set criteria?
* represent a more sophisticated “automated link checker” maintenance approach to changes in information?

Looking forwards we need to bear in mind that what we’re designing is not the only thing that such information provides value to. How do we design our data for re-use as agnostically as possible?

On top of this, it’s critical to acknowledge that the mechanics of some next steps could be complex, even if their central proposition is not. For example any given option could have multiple:

* eligibility criteria
* channels of delivery
* ways to join &ndash; such as a choice of memberships
* ways to use &ndash; such as access to different features based on membership

It follows that rather than attempt to recreate and maintain lots of complex content, we need to be able to do “just enough”. We must start lean in terms of how much information we display, and aim to meet essential needs first.

What is just enough for someone:

* to understand the proposition?
* to know how to start?
* to be engaged?

### 3. How well does filtering work?

If we acknowledge personalisation can only go so far, logically we need to create mechanisms to allow users explore beyond the initial set of options we present.

Do people understand the connection between the:

* questions we’ve asked
* results themselves
* available filters in the results listing?

How easy is it for the user to explore all available options?

### 4. Can we gauge intent?

A central piece of our proposition (and prevention strategy) is the idea of a feedback loop. We need to be able to check in and support people during their activities, playing the role of “interested friend”.

Yet again it is not news to state that “things are not joined up”. There is no consistent underlying capability that allows us to rely on “knowing via tech” what a user has decided (or not) to do next.

How can we know if a user has:

* downloaded and started to use an app?
* attended a community event?
* used a public facility?

## What we did

### Expanding the prototype user journey

We extended our user journey into how presenting next steps might work. We continued to iterate our onboarding segment, swapping out chunks to try variants or different approaches.

{% from "nhsuk/components/images/macro.njk" import image as nhsukImage %}
{{ nhsukImage({
  classes: "app-media--full-width",
  src: "user-journey-iterations@2x.png",
  alt: "Screen grabs displaying 3 iterations of our user journey. Each journey is shown from left to right. Each iteration appears below the previous.",
  caption: "3 iterations of our user journey"
}) }}

[Open a large version of this image (4.5mb)](user-journey-iterations-xlarge.jpg)

### Listings of next steps

For listings, we gradually moved from hard coded selections matched to participants’ local areas, to an API returning only national services derived from the [Better Health](https://www.nhs.uk/better-health/) website.

This enabled us to:

* continue to prove our “local is high value” hypotheses
* test a real dataset with real filters

{% from "nhsuk/components/images/macro.njk" import image as nhsukImage %}
{{ nhsukImage({
  classes: "app-media--full-width",
  src: "results-listing-iterations@2x.png",
  alt: "Screen grabs displaying 3 iterations of a results listing from left to right.",
  caption: "3 iterations of our results listing"
}) }}

### Details of a particular option

For pages showing details, we moved from:

* minimal and highly atomic content
* zero imagery
* a blocking approach requiring a declaration of intent

to:

* looser content retaining a strong structure
* minimal imagery
* a non-blocking approach to getting clues to intent

{% from "nhsuk/components/images/macro.njk" import image as nhsukImage %}
{{ nhsukImage({
  classes: "app-media--full-width",
  src: "service-detail-iterations@2x.png",
  alt: "Screen grabs displaying 2 iterations of a details page from left to right.",
  caption: "2 iterations of our details page"
}) }}

## What we learnt

![A sticker with the question 'has it got legs?'](has-it-got-legs@2x.png)

### Blend national and local

Since [discovery](/personalised-prevention-platform/2025/03/discovery-summary/) we’ve continuously proved that presenting a blend of national and local has real value to people. Throughout our sessions people asked if the options were real (they all were), and then make notes to look them up afterwards.

“National” and “local” are false distinctions, very visible to us, as we operate within organisational structures.

But where a thing “comes from” is utterly irrelevant to a user. You can be interested in Active 10, interested in your local Parkrun, and interested in the public gym in your local park.

### Strike a balance between needs and wants

Earlier onboarding prototypes included goal and priority setting segments, along with asking about barriers &ndash; things that could get in the way.

We removed these segments, instead asking a series of questions directly mapped to filters in the results listing, for example:

![A question page asking 'how do you like to be taught or coached?' alongside a column of filters displaying the same](example-filter-question@2x.png 'Onboarding questions mapped to filters')

This reductive approach led to some good evidence.

Reliance on asking preference alone means any “recommendation” or even “from left field” aspect is negated. We’re in pure service finder territory, and we’ve left no room for the unexpected or left-field that might spark engagement.

Some people told us they were basing their preferences on past experience &ndash; but that past experience was rooted in activities that had lapsed. So arguably we’re running a risk of simply presenting similar options to those that may have failed the user in the past.

We’ve proven that we need to become more opinionated in the options we present. These options must be mapped to a user’s declared goals, priorities, barriers, and preferences, but also weighted by our (systemic) opinion.

### Handle the relationship between volume, variety, and granularity

With 18 services in the “live” API, one or two users combined preferences that led to zero results. This led to immediate disengagement and in real life, dropoff.

There is a strong relationship between the volume and variety of information we hold, and the ability to tune (or “personalise”) a set of results. If you only have a few options to offer, you can only offer so much granular control in your interface.

Our 18 services represent a generic baseline that we know to be suitable for all areas and a wide range of people. In our pilot we expect to layer local offerings on top of this baseline, and so our volume and variety increases. With an idea of that increase, we get a better idea of how much granularity we can introduce.

Having a localised layer also allows us to practice ”no dead ends”. If our base selection is generic, then a minimum set of options would include all relevant generic options. For example a user’s priority to “exercise or move more” would at the absolute minimum return the Active 10 and Couch to 5k apps.

### “Engagement” can be simple

Unsurprisingly the early presentation of options was not engaging, with users often mentioning how unexciting they were.

What was surprising was how effective deliberately small tweaks were. The addition of a only small amount of imagery (in some cases only a logo) along with a looser content structure alleviated any further comment.

![A service result listing before and after the addition of a logo](logo-addition@2x.png 'Small visual tweaks had marked effect')

### Intent is the next big challenge

A big challenge for us is to figure out how and where in the overall journey we can find out what a user is actually doing.

It’s very easy in the abstract to miss interaction gotchas like this. We show the user the options, they pick one, then we check in later to see how it's going. Easy right?

Not so fast there. Let’s take Parkrun as an example. A potential user journey could be:

1. notice Parkrun in the listing
2. read more in the details and get interested
3. go to the Parkrun site to find out more (leaving our site, right?)
4. getting engaged and registering with Parkrun
5. attending their first event

At point 3 onwards, we will have no idea what they’re doing.

There’s two basic ways to approach this:

1. Gain a “declaration of intent” from the user.
2. Assemble as many clues and indications as we can during the user journey.

![Two screenshots, one showing an 'I want to do this' button, and the other showing app links and a 'what do you think?' question](intent-iteration@2x.png 'From blocking to gathering clues')

Our initial “blocking” design was essentially built with the expectation of failure. We insisted on a declaration: “I want to do this”

Wanted to test it (hey if it works then great, that would be neat) but also wanted to draw out solid reason why we should not do it. Tactics right.

In order to get a declaration of intent to use an option, we’re asking for an _immediate_ commitment from the user. It’s unrealistic to demand people must commit to a change this quickly, creating a fragility at a key point for us.

To create an interface that _demands_ a declaration means you have to withhold useful information (find out more about, directions, any contact details). If you have a single high importance CTA that you are massively relying on, you cannot provide routes around it, you are aiming to strongly funnel users.

Risk of failure risk to the actual proposition

All this aside, users were confused. Lots of people didn't see or understand what the CTA was for. When prompted to explain what it was answers varied from things like "it would launch the app right?" to "it would display more details" (correct).

Even if you somehow nail this (unlikely - remember the service information problem), the risk of false positives remains.

Clues and indicators both via tracking and via opportunity. None of which is a point of failure.

---

Without having a decent idea of even the nature of the option chosen - is it an app, in perosn, how long, what's the interval - our aboility tj follow up is hamperd or personalised that follow uto

Think “patient reported outcomes” for example.

We’ve got ideas about how to glean intent from various clues in this part of the journey, and as we produce our check-ins we’ll be stress testing them.

(Epilogue) What we’re doing next

- Latest work is around "the very first check in"
- Jumping the gap between presenting the options and figuring out if something's being done

- writing up something about intent in the recommnedations bit
- writing up somethibg about service display
